{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c0cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/docker\n"
     ]
    }
   ],
   "source": [
    "# Add /usr/local/bin to PATH so that docker is found (required for agent code execution)\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/bin\"\n",
    "\n",
    "import shutil\n",
    "print(shutil.which(\"docker\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9371b",
   "metadata": {},
   "source": [
    "# The dataset to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a396647",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"original_dataset\": \"../data/member_employers.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b544fc",
   "metadata": {},
   "source": [
    "# Imports, Environment Variables, Tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456eaab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stewartrobertson/crewAI/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Process, Crew\n",
    "from dotenv import load_dotenv\n",
    "from crewai_tools import FileReadTool, FileWriterTool, DirectoryReadTool\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Specify the model. Comment out to use default (gpt-4o)\n",
    "os.environ[\"OPENAI_MODEL_NAME\"] = 'gpt-4.1-mini'\n",
    "\n",
    "csv_reader = FileReadTool(file_path=inputs[\"original_dataset\"])\n",
    "file_writer = FileWriterTool()\n",
    "directory_reader = DirectoryReadTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ed380",
   "metadata": {},
   "source": [
    "## Creating Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab719a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first agent will ingest the dataset, identify and report all quality issues, and provide strategies to resolve them\n",
    "data_cleaning_agent = Agent(\n",
    "    role=\"Senior Data Engineer\",\n",
    "    goal=\"\"\"\n",
    "        You are a Senior Data Engineer specialising in data ingestion and cleaning. \n",
    "        Your primary goal is to ingest the raw data provided, identify all quality issues, and produce python code to resolve all identified quality issues.\n",
    "\n",
    "        Responsibilities:\n",
    "        1. Ingest the complete raw dataset\n",
    "        2. Develop python code to resolve all data quality issues\n",
    "        3. Adhere to data engineering best practices\n",
    "        \"\"\",\n",
    "    backstory=\"Senior Data Engineer experienced in Python, SQL, data cleaning techniques, and producing top-tier python code.\",\n",
    "    allow_code_execution=True,\n",
    "    memory=True,\n",
    "    verbose=True,\n",
    "    tools=[csv_reader, file_writer, directory_reader],\n",
    "    #llm=claude_llm\n",
    ")\n",
    "\n",
    "# The second agent will perform exploratory data analysis on the dataset to derive insights and suggest further areas of analysis\n",
    "eda_agent = Agent(\n",
    "    role=\"Senior Data Scientist\",\n",
    "    goal=\"\"\"\n",
    "        You are a Senior Data Scientist with deep expertise in exploratory data analysis and statistical analysis.\n",
    "        \n",
    "        Responsibilities\n",
    "        1. Ingest the complete raw dataset to perform analysis\n",
    "        2. Develop python code to:\n",
    "            - calculate various statistics\n",
    "            - perform correlation analysis\n",
    "            - perform distribution analysis\n",
    "            - perform pattern analysis\n",
    "        3. Adhere to data science best practices\n",
    "        \"\"\",\n",
    "    backstory=\"A senior data scientist with extensive experience in exploratory data analysis and using python, pandas, and data manipulation.\",\n",
    "    allow_code_execution=True,\n",
    "    memory=True,\n",
    "    verbose=1,\n",
    "    tools=[csv_reader, file_writer, directory_reader],\n",
    "    #llm=openai_llm\n",
    ")\n",
    "\n",
    "# The third agent will create a report on suggested data visualisations, and use python to visualise and save visualisations of the data\n",
    "data_visualisation_agent = Agent(\n",
    "    role=\"Senior Data Visualisation Engineer\",\n",
    "    goal=\"\"\"\n",
    "        You are a Senior Data Visualisation Engineer with expertise in creating impactful visualisations that provide clear insight into data.\n",
    "        \n",
    "        Responsibilities:\n",
    "        1. Ingest the complete raw dataset\n",
    "        2. Generate python code to create visualisations of the data following data visualisation best practices\n",
    "        \"\"\",\n",
    "    backstory=\"A senior data visualisation engineer with extensive experience in creating high-quality visualitsations using python.\",\n",
    "    allow_code_execution=True,\n",
    "    memory=True,\n",
    "    verbose=1,\n",
    "    tools=[file_writer, directory_reader],\n",
    "    #llm=openai_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf829689",
   "metadata": {},
   "source": [
    "## Creating Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7a07b",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a5bea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The data cleaning tasks will be performed in sequence by the data_cleaning_agent \"\"\"\n",
    "\n",
    "data_cleaning_process = [\n",
    "    # Task 1: Initial Data Assessment\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Develop python code to conduct an initial assessment of the {original_dataset}. Ingest the first 500 rows.\n",
    "            Specifically:\n",
    "            1. Identify all columns and their data types\n",
    "            2. Check for basic data quality issues\n",
    "            3. Be as thorough and detailed as possible\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A python code file in a folder called 'cleaning_files/' containing code to calculate:\n",
    "                - Data quality metrics:\n",
    "                    - Number of rows\n",
    "                    - Number of columns\n",
    "                    - Number of missing values\n",
    "                    - Number of duplicates\n",
    "                    - Data types\n",
    "                \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"initial_data_assessment.py\",\n",
    "    ),\n",
    "\n",
    "    # Task 2: Missing Value Handling\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} and crate a python script to handle missing values present, if there are any.\n",
    "            Specifically:\n",
    "            1. Ingest the file using pandas\n",
    "            2. Check for missing values in the dataset\n",
    "            3. Determine an appropriate handling strategy and write code to implement it\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            A python code file in a folder called 'cleaning_files/' containing code to handle missing values present, if there are any.\n",
    "            \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"missing_value_handling.py\",\n",
    "    ),\n",
    "    \n",
    "    # Task 3: Format Standardisation\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} and develop a python script to standardise formats across the dataset, if formats are not standard.\n",
    "            Specifically:\n",
    "            1. Use the pandas library to read the file and perform data manipulation\n",
    "            2. Identify inconsistent formats\n",
    "            3. Develop a standardisation strategy (e.g., date formats, string formats, etc.)\n",
    "            4. Write code to implement the strategy\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            A python code file in a folder called 'cleaning_files/' containing code to standardise formats across the dataset, if formats are not standard.\n",
    "            \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"format_standardisation.py\",\n",
    "    ),\n",
    "    \n",
    "    # Task 4: Duplicate Handling\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} and develop a python script to handle duplicates in the dataset, if there are any.\n",
    "            Specifically:\n",
    "            1. Use the pandas library to read the file\n",
    "            2. Develop a strategy for handling duplicates.\n",
    "            3. Write code to implement the strategy\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            A python code file in a folder called 'cleaning_files/' containing code to handle duplicates in the dataset, if there are any.\n",
    "            \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"duplicate_handling.py\",\n",
    "    ),\n",
    "    \n",
    "    # Task 5: Outlier Handling\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} and develop a python script to handle outliers in the dataset.\n",
    "            Specifically:\n",
    "            1. Use the pandas library to read the file\n",
    "            2. Identify outliers using statistical methods (e.g., Z-score, IQR)\n",
    "            3. Develop a strategy for handling outliers (e.g., removal, transformation, etc.)\n",
    "            4. Write code to implement the strategy\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            A python code file in a folder called 'cleaning_files/' containing code to handle outliers in the dataset.\n",
    "            \"\"\",\n",
    "        agent=data_cleaning_agent,\n",
    "        output_file=\"outlier_handling.py\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ecf6c",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f41f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The exploratory data analysis tasks will be performed in sequence by the eda_agent \"\"\"\n",
    "\n",
    "eda_process = [\n",
    "    # Task 1: Basic Statistics\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} file and develop a python script to calculate statistics for the dataset.\n",
    "            Specifically the code should:\n",
    "            1. Calculate descriptive statistics for numerical columns (where it makes sense)\n",
    "            2. Calculate descriptive statistics for categorical columns (where it makes sense) \n",
    "            3. Be as thorough and detailed as possible\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A python code file in a folder called 'eda_files/' containing code to calculate basic statistics for the dataset.\n",
    "            \"\"\",\n",
    "        agent=eda_agent,\n",
    "        output_file=\"eda_statistics.py\",\n",
    "        #output_pydantic=BasicStatistics\n",
    "    ),\n",
    "    \n",
    "    # Task 2: Correlation Analysis\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} file and develop a python script to calculate correlations for the dataset.\n",
    "            Specifically the code should:\n",
    "            1. Calculate correlations\n",
    "            2. Be as thorough and detailed as possible but do do not do any further analysis\n",
    "            3. Infer any possible important insights from the correlations\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),        \n",
    "        expected_output=\"\"\"\n",
    "            - A python code file in a folder called 'eda_files/' containing code to calculate correlations for the dataset.\n",
    "            \"\"\",\n",
    "        agent=eda_agent,\n",
    "        output_file=\"eda_correlation.py\",\n",
    "        #output_pydantic=CorrelationAnalysis\n",
    "    ),\n",
    "    \n",
    "    # Task 3: Distribution Analysis\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} file and develop a python script to analyse variable distributions.\n",
    "            Specifically the code should:\n",
    "            1. Calculate distributions\n",
    "            2. Identify outliers\n",
    "            3. Be as thorough and detailed as possible but do do not do any further analysis\n",
    "            4. Infer any possible important insights from the distributions\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A python code file in a folder called 'eda_files/' containing code to analyse variable distributions.\n",
    "            \"\"\",\n",
    "        agent=eda_agent,\n",
    "        output_file=\"eda_distribution.py\",\n",
    "        #output_pydantic=DistributionAnalysis\n",
    "    ),\n",
    "    \n",
    "    # Task 4: Pattern Identification\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} file and develop a python script to identify patterns and trends.\n",
    "            Specifically the code should:\n",
    "            1. Look for temporal patterns\n",
    "            2. Identify clusters\n",
    "            3. Be as thorough and detailed as possible but do do not do any further analysis\n",
    "            4. Infer any possible important insights from the patterns\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            - A python code file in a folder called 'eda_files/' containing code to identify patterns and trends.\n",
    "            \"\"\",\n",
    "        agent=eda_agent,\n",
    "        output_file=\"eda_patterns.py\",\n",
    "        #output_pydantic=PatternIdentification\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176a96d",
   "metadata": {},
   "source": [
    "### Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0c7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The data visualisation tasks will be completed in sequence by the data_visualisation_agent \"\"\"\n",
    "\n",
    "# Visualisation Tasks\n",
    "visualisation_process = [\n",
    "    Task(\n",
    "        description=\"\"\"\n",
    "            Ingest the first 500 rows of {original_dataset} and generate a python script that will generate various detailed visualisations.\n",
    "            Specifically:\n",
    "            1. Ingest the first 500 rows of {original_dataset} file using pandas\n",
    "            2. Use seaborn for visualisation\n",
    "            3. Be creative and thoughtful with what charts to create\n",
    "            4. Be thorough and make an extensive list of visualisations\n",
    "            4. Include legends and titles\n",
    "            \"\"\".format(original_dataset=inputs[\"original_dataset\"]),\n",
    "        expected_output=\"\"\"\n",
    "            A python code file in a folder called visualisation_files/ containing the code to generate visualisations as per the task.\n",
    "            \"\"\",\n",
    "        agent=data_visualisation_agent,\n",
    "        output_file=\"visualisations.py\"\n",
    "        #output_pydantic=VisualisationCreation\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c2a3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the tasks into a single process\n",
    "all_tasks = data_cleaning_process + eda_process + visualisation_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60358a9",
   "metadata": {},
   "source": [
    "## Create the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eaf5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "data_analysis_crew = Crew(\n",
    "    agents=[\n",
    "        #data_cleaning_agent,\n",
    "        #eda_agent,\n",
    "        data_visualisation_agent,\n",
    "    ],\n",
    "\n",
    "    #tasks=all_tasks,\n",
    "    tasks=visualisation_process,\n",
    "    manager_llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2), # try to make the manager more deterministic\n",
    "    process=Process.hierarchical,\n",
    "    verbose=False,\n",
    "    memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccb951a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to give feedback on task completions\n",
    "#data_analysis_crew.train(n_iterations=1, filename=\"crew_training.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bbe9d",
   "metadata": {},
   "source": [
    "## Kickoff the crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bca3878b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Visualisation Engineer\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mCreate a Python script to generate various detailed visualisations from the first 500 rows of ../data/member_employers.csv using pandas and seaborn.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Visualisation Engineer\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mAction: File Writer Tool\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mFile Writer Tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"filename\\\": \\\"visualisation_member_employers_20240625.py\\\", \\\"directory\\\": null, \\\"overwrite\\\": true, \\\"content\\\": \\\"import pandas as pd\\\\nimport seaborn as sns\\\\nimport matplotlib.pyplot as plt\\\\nimport os\\\\nimport argparse\\\\n\\\\n# Set up argument parser for flexibility in input and output paths\\\\nparser = argparse.ArgumentParser(description='Visualize member employers dataset')\\\\nparser.add_argument('--input', type=str, default='../data/member_employers.csv', help='Input CSV file path')\\\\nparser.add_argument('--output_dir', type=str, default='visualisation_files/plots', help='Output directory for plots')\\\\nargs = parser.parse_args()\\\\n\\\\n# Ingest the first 500 rows of the CSV file\\\\nfile_path = args.input\\\\ndf = pd.read_csv(file_path, nrows=500)\\\\n\\\\n# Basic data inspection (printed for user to understand the data)\\\\nprint(\\\\\\\"Data head:\\\\\\\")\\\\nprint(df.head())\\\\nprint(\\\\\\\"Data info:\\\\\\\")\\\\nprint(df.info())\\\\nprint(\\\\\\\"Data description:\\\\\\\")\\\\nprint(df.describe(include='all'))\\\\n\\\\n# Set seaborn style for clarity and good aesthetics\\\\nsns.set(style='whitegrid')\\\\n\\\\n# Create output directory if it does not exist\\\\noutput_dir = args.output_dir\\\\nos.makedirs(output_dir, exist_ok=True)\\\\n\\\\n# Helper function to save plots robustly\\\\n\\\\ndef save_plot(fig, name):\\\\n    filename = os.path.join(output_dir, f\\\\\\\"{name}.png\\\\\\\")\\\\n    fig.savefig(filename, bbox_inches='tight')\\\\n    plt.close(fig)\\\\n\\\\n# 1. Distribution histograms for numeric columns with KDE\\\\nnumeric_cols = df.select_dtypes(include=['number']).columns.tolist()\\\\nfor col in numeric_cols:\\\\n    fig, ax = plt.subplots(figsize=(8,5))\\\\n    sns.histplot(df[col], kde=True, ax=ax, color='blue')\\\\n    ax.set_title(f'Distribution of {col}')\\\\n    ax.set_xlabel(col)\\\\n    ax.set_ylabel('Frequency')\\\\n    save_plot(fig, f'distribution_{col}')\\\\n\\\\n# 2. Boxplots for numeric columns to identify outliers\\\\nfor col in numeric_cols:\\\\n    fig, ax = plt.subplots(figsize=(8,5))\\\\n    sns.boxplot(x=df[col], ax=ax, color='orange')\\\\n    ax.set_title(f'Boxplot of {col}')\\\\n    save_plot(fig, f'boxplot_{col}')\\\\n\\\\n# 3. Countplots for categorical columns with reasonable cardinality\\\\ncat_cols = df.select_dtypes(exclude=['number']).columns.tolist()\\\\nfor col in cat_cols:\\\\n    unique_values = df[col].nunique()\\\\n    if 1 < unique_values < 30:\\\\n        fig, ax = plt.subplots(figsize=(10,6))\\\\n        sns.countplot(y=col, data=df, order=df[col].value_counts().index, ax=ax, palette='viridis')\\\\n        ax.set_title(f'Countplot of {col}')\\\\n        save_plot(fig, f'countplot_{col}')\\\\n\\\\n# 4. Correlation heatmap for numeric features\\\\nif len(numeric_cols) > 1:\\\\n    corr = df[numeric_cols].corr()\\\\n    fig, ax = plt.subplots(figsize=(10,8))\\\\n    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', ax=ax)\\\\n    ax.set_title('Correlation Heatmap of Numeric Features')\\\\n    save_plot(fig, 'correlation_heatmap')\\\\n\\\\n# 5. Pairplot for numeric features to explore relationships\\\\nif len(numeric_cols) > 1:\\\\n    pairplot = sns.pairplot(df[numeric_cols].dropna())\\\\n    pairplot.fig.suptitle('Pairplot of Numeric Features', y=1.02)\\\\n    pairplot.savefig(os.path.join(output_dir, 'pairplot_numeric_features.png'))\\\\n    plt.close()\\\\n\\\\n# 6. Bar plots of average numeric values grouped by categorical columns\\\\nfor cat_col in cat_cols:\\\\n    unique_values = df[cat_col].nunique()\\\\n    if 1 < unique_values < 30:\\\\n        for num_col in numeric_cols:\\\\n            grouped = df.groupby(cat_col)[num_col].mean().sort_values(ascending=False)\\\\n            fig, ax = plt.subplots(figsize=(10,6))\\\\n            sns.barplot(x=grouped.values, y=grouped.index, ax=ax, palette='magma')\\\\n            ax.set_title(f'Average {num_col} by {cat_col}')\\\\n            ax.set_xlabel(f'Average {num_col}')\\\\n            ax.set_ylabel(cat_col)\\\\n            save_plot(fig, f'avg_{num_col}_by_{cat_col}')\\\\n\\\\n# 7. Countplots showing relationship between two categorical variables if available\\\\nif len(cat_cols) >= 2:\\\\n    cat1 = cat_cols[0]\\\\n    cat2 = cat_cols[1]\\\\n    fig, ax = plt.subplots(figsize=(12,8))\\\\n    sns.countplot(data=df, x=cat1, hue=cat2, ax=ax, palette='Set2')\\\\n    ax.set_title(f'Countplot of {cat1} grouped by {cat2}')\\\\n    plt.xticks(rotation=45)\\\\n    save_plot(fig, f'countplot_{cat1}_by_{cat2}')\\\\n\\\\n# Suggestion: To explore interactivity in the future, consider using plotly.express or bokeh libraries\\\\n\\\\nprint(f\\\\\\\"Visualizations created and saved to the '{output_dir}' directory.\\\\\\\")\\\\n\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "Content successfully written to visualisation_member_employers_20240625.py\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Data Visualisation Engineer\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import os\n",
      "import argparse\n",
      "\n",
      "# Set up argument parser for flexibility in input and output paths\n",
      "parser = argparse.ArgumentParser(description='Visualize member employers dataset')\n",
      "parser.add_argument('--input', type=str, default='../data/member_employers.csv', help='Input CSV file path')\n",
      "parser.add_argument('--output_dir', type=str, default='visualisation_files/plots', help='Output directory for plots')\n",
      "args = parser.parse_args()\n",
      "\n",
      "# Ingest the first 500 rows of the CSV file\n",
      "file_path = args.input\n",
      "df = pd.read_csv(file_path, nrows=500)\n",
      "\n",
      "# Basic data inspection (printed for user to understand the data)\n",
      "print(\"Data head:\")\n",
      "print(df.head())\n",
      "print(\"Data info:\")\n",
      "print(df.info())\n",
      "print(\"Data description:\")\n",
      "print(df.describe(include='all'))\n",
      "\n",
      "# Set seaborn style for clarity and good aesthetics\n",
      "sns.set(style='whitegrid')\n",
      "\n",
      "# Create output directory if it does not exist\n",
      "output_dir = args.output_dir\n",
      "os.makedirs(output_dir, exist_ok=True)\n",
      "\n",
      "# Helper function to save plots robustly\n",
      "\n",
      "def save_plot(fig, name):\n",
      "    filename = os.path.join(output_dir, f\"{name}.png\")\n",
      "    fig.savefig(filename, bbox_inches='tight')\n",
      "    plt.close(fig)\n",
      "\n",
      "# 1. Distribution histograms for numeric columns with KDE\n",
      "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
      "for col in numeric_cols:\n",
      "    fig, ax = plt.subplots(figsize=(8,5))\n",
      "    sns.histplot(df[col], kde=True, ax=ax, color='blue')\n",
      "    ax.set_title(f'Distribution of {col}')\n",
      "    ax.set_xlabel(col)\n",
      "    ax.set_ylabel('Frequency')\n",
      "    save_plot(fig, f'distribution_{col}')\n",
      "\n",
      "# 2. Boxplots for numeric columns to identify outliers\n",
      "for col in numeric_cols:\n",
      "    fig, ax = plt.subplots(figsize=(8,5))\n",
      "    sns.boxplot(x=df[col], ax=ax, color='orange')\n",
      "    ax.set_title(f'Boxplot of {col}')\n",
      "    save_plot(fig, f'boxplot_{col}')\n",
      "\n",
      "# 3. Countplots for categorical columns with reasonable cardinality\n",
      "cat_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
      "for col in cat_cols:\n",
      "    unique_values = df[col].nunique()\n",
      "    if 1 < unique_values < 30:\n",
      "        fig, ax = plt.subplots(figsize=(10,6))\n",
      "        sns.countplot(y=col, data=df, order=df[col].value_counts().index, ax=ax, palette='viridis')\n",
      "        ax.set_title(f'Countplot of {col}')\n",
      "        save_plot(fig, f'countplot_{col}')\n",
      "\n",
      "# 4. Correlation heatmap for numeric features\n",
      "if len(numeric_cols) > 1:\n",
      "    corr = df[numeric_cols].corr()\n",
      "    fig, ax = plt.subplots(figsize=(10,8))\n",
      "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', ax=ax)\n",
      "    ax.set_title('Correlation Heatmap of Numeric Features')\n",
      "    save_plot(fig, 'correlation_heatmap')\n",
      "\n",
      "# 5. Pairplot for numeric features to explore relationships\n",
      "if len(numeric_cols) > 1:\n",
      "    pairplot = sns.pairplot(df[numeric_cols].dropna())\n",
      "    pairplot.fig.suptitle('Pairplot of Numeric Features', y=1.02)\n",
      "    pairplot.savefig(os.path.join(output_dir, 'pairplot_numeric_features.png'))\n",
      "    plt.close()\n",
      "\n",
      "# 6. Bar plots of average numeric values grouped by categorical columns\n",
      "for cat_col in cat_cols:\n",
      "    unique_values = df[cat_col].nunique()\n",
      "    if 1 < unique_values < 30:\n",
      "        for num_col in numeric_cols:\n",
      "            grouped = df.groupby(cat_col)[num_col].mean().sort_values(ascending=False)\n",
      "            fig, ax = plt.subplots(figsize=(10,6))\n",
      "            sns.barplot(x=grouped.values, y=grouped.index, ax=ax, palette='magma')\n",
      "            ax.set_title(f'Average {num_col} by {cat_col}')\n",
      "            ax.set_xlabel(f'Average {num_col}')\n",
      "            ax.set_ylabel(cat_col)\n",
      "            save_plot(fig, f'avg_{num_col}_by_{cat_col}')\n",
      "\n",
      "# 7. Countplots showing relationship between two categorical variables if available\n",
      "if len(cat_cols) >= 2:\n",
      "    cat1 = cat_cols[0]\n",
      "    cat2 = cat_cols[1]\n",
      "    fig, ax = plt.subplots(figsize=(12,8))\n",
      "    sns.countplot(data=df, x=cat1, hue=cat2, ax=ax, palette='Set2')\n",
      "    ax.set_title(f'Countplot of {cat1} grouped by {cat2}')\n",
      "    plt.xticks(rotation=45)\n",
      "    save_plot(fig, f'countplot_{cat1}_by_{cat2}')\n",
      "\n",
      "# Suggestion: To explore interactivity in the future, consider using plotly.express or bokeh libraries\n",
      "\n",
      "print(f\"Visualizations created and saved to the '{output_dir}' directory.\")\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = data_analysis_crew.kickoff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
